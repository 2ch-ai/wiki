✱ **AdamW** - база для хорошего выбора любой тренировки картинкогенерирующих сетей. Имеет 8-битную версию для экономия памяти, работает на ура со всеми архитектурами от 1.5/XL до 3.5/flux. Большая часть существующих гайдов расписывают параметры для него, в том числе и рекомендуемые лр'ы. Дефолтные параметры - `"weight_decay=0.01" "betas=0.9,0.999"`. Дефолтный лр с базовыми лорами для юнета около **1e-4**.

✱ **Prodigy** - адаптивный оптимайзер, имеющий в основе адама. Сам регулирует лр, который с ним надо ставить **1.0**. Дефолтные параметры - `"decouple=True" "weight_decay=0.01" "d_coef=1"`. `d_coef` адаптивный множитель лра, для его регулирования. Лр лучше крутить с помощью него. Тренит медленнее обычного адама, так же потребляет больше памяти. Имеет [очень жирный форк](https://github.com/LoganBooker/prodigy-plus-schedule-free) с кучей новых фич, самым же минимумом будет разделение лр'ов из [этого пр'а](https://github.com/konstmish/prodigy/pull/20) при тренировке с энкодером. Вторая отличная база, если хватает ресурсов, для любой юнет онли тренировки среди всех архитектур, с энкодерами же есть нюансы, и разделение лр очень рекомендуется к применению. Форк, кстати, имеет оптимизации для врам.

✱ **Adafactor** - врам френдли оптимайзер, единственный который позволит с 8-12 гигами [тренить полноценно XL файнтюн](https://rentry.org/lora-is-not-a-finetune). В остальном, сливает адаму, да и тем более продиджи. Дефолт аргументы для врам френдли мода - `"scale_parameter=False" "relative_step=False" "warmup_init=False" --fused_backward_pass`.