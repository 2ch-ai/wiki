# KoboldCpp

Гайд для ~~ретардов~~ быстрого вката в LLaMA без излишней ебли.

1. Качаешь [koboldcpp.exe последней версии](https://github.com/LostRuins/koboldcpp/releases/). 
2. Качаешь [одну из моделей](https://huggingface.co/TheBloke/Frostwind-10.7B-v1-GGUF/blob/main/frostwind-10.7b-v1.Q5_K_M.gguf)
3. Запускаем koboldcpp.exe и выбираем скачанную модель.
4. Заходим в браузере на <http://localhost:5001/>
Все, общаемся с ИИ, читаем охуительные истории или отправляемся в Adventure.

Да, просто запускаем, выбираем файл и открываем адрес в браузере, даже ваша бабка разберется!

# Какую модель качать?
Вывод кобольда (как и качество кумирования) целиком и полностью зависит от выбранной модели. Модель это плоть и кровь вашей нейроночки, это то, какими знаниями и навыками она обладает. Существуют как специализированные модели, так и общие, которые умеют всё понемногу. Есть даже модели, которые являются миксами двух и более существующих.

Модели делятся по **колличеству параметров** (в миллиардах, сокращённо "B") и по **степени квантования**.

**Количество параметров** приблизительно передаёт "вес" модели. Чем больше весит модель, тем она круче и умнее (но не всегда). Разница в качестве выдаваемого текста у 7B модели и 34B модели колоссальная, но вместе с обьёмом возрастает также и требования к памяти под модель, и к мощности пекарни, так что не удивляйтесь если на вашей 1050Ti моделька на 34B будет тратить на слово несколько секунд. На странице модели будет указано примерное потребление оперативки моделью. Эту память можно распределить между своей оперативкой и VRAM видеокарты. Говорят, если полностью загрузить модель в память видюхи, она будет работать побыстрее. Но даже если у вас памяти впритык, то всё равно можно попробовать, тогда модель будет использовать файл подкачки, из-за чего скорость упадёт в разы.

**Степень квантования** (чаще всего сокращают до Q_3 или Q5_K_M) это "сжатие модели", потипу lossy видеофайлов. Квантование может сильно уменьшить вес модели, при этом сохраняя её параметры, но модель от этого становится тупее. Чем меньше число после Q_, тем меньше квант, а значит, и меньше её качество. Иногда сильно квантованные модели просто неюзабельны из-за их тупости, как порнуха в 240p, однако на кванах 5 и больше потери качества не такие существенные. Говорят, на квантах Q_3 существует жизнь, но это не доказано.

Ну чтож, анон, давай выберем тебе модельку из самых популярных у анонов.  
- **Если всё супер плохо** и ты сидишь в деревне с бабушкиной пекарни для созвонов в скайпе, качай [OpenHermes-2.5-Mistral-7B](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/blob/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf).  
- **Если у тебя 8гб оперативы и видеокарта уровня 1050ti (или 16гб оперативы и нет видеокарты)**, качай [Frostwind-10.7B](https://huggingface.co/TheBloke/Frostwind-10.7B-v1-GGUF/blob/main/frostwind-10.7b-v1.Q5_K_M.gguf) или [OpenHermes-2.5-Mistral-7B](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/blob/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf).  
- **Если у тебя топовый егровой пека, 32гб оперативы и видеокарта уровня 3060TI/6600XT**, качай [Noromaid-20B](https://huggingface.co/TheBloke/Noromaid-20B-v0.1.1-GGUF/blob/main/noromaid-20b-v0.1.1.Q5_K_M.gguf) или [MLewd-ReMM-L2-Chat-20B](https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B-GGUF/blob/main/MLewd-ReMM-L2-Chat-20B.q5_K_M.gguf).  
- Для специализированных сборок с видюхами майнинг-уровня, вроде NVidia P40 24G можешь попробовать модельки на 70B. Они несколько круче 34B, но не сказать чтобы прям очень сильно, но зато тебе не придется ждать часами одного ответа.  
- Говорят, существует моделька на 180B, но, поверь, тебе это не нужно.

Если же ты решил сам выбрать себе модельку, ищи на huggingface файлы с расширением GGUF. Вот у этого челика есть [огромный список переконвертированных в GGUF моделей](https://huggingface.co/TheBloke?search_models=GGUF), пользуйся поиском.
А вот [страничка с рейтингом моделей для кумирования](http://ayumi.m8geil.de/ayumi_bench_v3_results.html), впрочем, основаны результаты не на отзывах юзеров, а _примерно почувствованы_, так что не панацея, спрашивай советов в треде.

И не забывай что иногда модель может показать себя очень круто, а иногда выдавать банальности и кринге, так что не дропай модели сразу после того как она обосрётся на поцелуе.

# Всё очень медленно работает!
При запуске KoboldCPP можно выбрать пресет для BLAS. BLAS — пакет библиотек, ответственный за "пережёвывание" твоего промпта (в том числе, разборка уже написаного текста на токены). Имеется несколько вариантов реализации BLAS:

По умолчанию выбран **openBLAS**. openBLAS это реализация BLAS для работы на CPU, и если у тебя нет видюхи (или по каким-то причинам ты не хочешь её использвать), выбирай его.

Если у тебя есть видеокарта, то для заметного ускорения работы, лучше использовать её. Ты можешь выгрузить на неё часть слоёв модели. Для этого в дропдауне окошка KoboldCPP, там, где написано openBLAS, нужно выбрать вариант под свою видюху.

Если у тебя NVidia, выберай **CuBLAS**. 
Если у тебя Radeon вместо видеокарты, есть смысл попробовать [СБОРКУ ПОД ROCm](https://github.com/YellowRoseCx/koboldcpp-rocm/releases), но ROCm под винду доступен только для 6800+ видеокарт. В списке появится **rocmBlas**, выбери его.

Если же у тебя радеон постарее, например 6600XT, выберай **clBlast**, он тоже будет использовать твою видеокарту, значительно ускорив работу нейронки.

Далее тебе нужно выбрать в каком слоте стоит видеокарта (в дропдауне, 1-4) и количество слоёв, которые туда нужно выгрузить. Выгруженные в видюху слои работают быстрее, а ещё они не жрут твою оперативу, позволяя тебе побыстрее генерить. Количество слоёв для выгрузки определяется индивидуально, экспериментами. В диспетчере задач погляди загруженность видюхи и выставляй больше или меньше по своему усмотрению. На нвидии, кстати, не стоит забивать память на фулл, говорят там будет всё глючить.

Если выбрать 0 слоёв для GPU, то текст писаться быстрее не будет, но на этапе precessing BLAT ускорение всё же будет.

# Сетка забыла что вообще происходит!
Так, смотри, память сетки ограничена размером контекста. Контекст это те слова (точнее, токены), которые ей скормлены. У большинства моделей для LLaMA2 максимальный размер контекста - 4096 токенов. Если ставить больше, сетка может ебануться и начать писать ахинею. Размер контекста ты самостоятельно выбираешь в лаунчере кобольда, чем он больше, тем больше он займёт оперативы, причем требования возрастают квадратично.

При повышении контекста, в зависимости от модели нужно будет изменять и один из параметров RoPE scale, или RoPE base. Некоторые файнтюны имеют кастомный RoPE base, который нужно выставлять таким, который указан в конфиге моделей, лежащем рядом с ней на обниморде. В ином же случае стоит придерживаться [значений из вики](https://github.com/LostRuins/koboldcpp/wiki#what-is-rope-config-what-is-ntk-aware-scaling--what-values-to-use-for-rope-config) для растягивания контекста, выше базовых 4к у ламы2.

Контекст это вообще всё что выгружено в память нейронки, на основе чего она генерирует слова. В него входит история сообщений в чате, **Memory**, **Author's Note** и **World Info**.

По мере чата/игры/написания истории сетка будет всё меньше и меньше обращать внимание на старые сообщения, а спустя некоторое время (после того как они окажутся дальше чем Context Size) они просто пропадут из её памяти.

Чтобы как-нибудь расширить память нейронки, существует вкладка **Memory**. В неё нужно коротко записывать основные моменты истории, чтобы не лишать аниме-тянок девственности по два раза за игру. Мемори запихивается в самое начало контекста и имеет довольно мало влияния на генерацию.

Также чтобы сетка запомнила, например, о том как выглядит и какой характер у той или иной тянки, или чтобы доучить её какому-нибудь понятию о мире, которого в ней изначально не было, нужно использовать **World Info**. На этой вкладке ты указываешь слова и их ассоциации для нейронки. Например, ты можешь создать ключ "Melissa" и описать его как "Melissa is a petite 19-year old girl wearing glasses with irritated behavior. She often gets angry at everybody but can me easily calmed. Melissa and Kat are best friends.". Теперь, после упоминания Мелиссы сетка получит описанные токены и будет генерировать текст на основе них. В описание ключа также нужно вставить само имя ключа, то есть не "A girl..." а "Melissa is a girl...", так как сеть не включает само название ключа в контекст.

Ну и самое важное, **Author's Note** это такое поле, которое будет кормиться сетке перед каждым запросом. То есть, буквально, напоминать ей что именно от неё ждут, в целом. Тут лучше всего написать какой должен быть стиль повествования, краткие сведения о мире и прям совсем общие положения, например, "The horror story where the young campers are being hunted and killed in the woods by an unknown maniac". Также, у Author's note можно указать дальность от текста, которая работает как "Важность" указанной информации.

[>> Подробнее про долговременную память](https://github.com/KoboldAI/KoboldAI-Client/wiki/Memory,-Author's-Note-and-World-Info)

#Мне лень каждый раз выбирать нужное в лаунчере

Итак, ты уже разобрался с лаунчером, знаешь где что тыкать, какую модельку загружать, сколько слоёв куда ставить. Давай ускорим запуск кобольда с помощью командной строки.

Значит так, жмёшь пкм на ярлыке кобольда и дописываешь в поле "Обьект" что-то типа следующего:`C:\NeuralNet\LLaMA\koboldcpp.exe --model frostwind-10.7b-v1.Q5_K_M.gguf --port 5001`, это запустит консольку кобольда сразу же, без запуска UI, загрузит модель в неё и начнет работу. Все параметры, которые ты вбиваешь в UI, можно указать и в консоли. Например, запуск кобольда с параметрами `C:\NeuralNet\LLaMA\koboldcpp.exe --model frostwind-10.7b-v1.Q5_K_M.gguf --port 5001 --contextsize 4096  --useclblast 0 1  --gpulayers 28` загрузит модель Frostwind, откроет под себя порт localhost:5001, задаст размер контекста в 4096, будет использовать clBlast (тут методом тыка нужно выяснить где где стоит видюха, можешь попробовать 0 0 или 1 0, пробуй пока не она не определится), и выгрузит на неё 28 слоёв. Удобно же.

Полный список команд можно подсмотреть запустив кобольд из консоли с командой --help. Вот он:

**Полный список консольных команд**
<details><summary>Раскрыть</summary>

| Команда | Описание |
| ------ | ------ |
|-h, --help|Выводит этот список|
|--model [MODEL] |Модель для загрузки. Можно указать локальный путь, если моделька лежит рядом с экзешником.|
|--port PORT |Порт для вывода, например 5001|
|--host HOST|Host IP to listen on. If empty, all routable interfaces are accepted.|
|--launch|Откроет а браузере вкладку с кобольдом после запуска|
|--lora [lora_filename] [[lora_base] ...]|Лора для загрузки поверх модели, работает только с LLAMA моделями. Эксперимнтальная фича.|
|--config CONFIG|Можно загрузить файл с настройками кобольда. Настройки можно сохранить в UI, жмякнув Save внизу. При этом все остальные аргументы будут игниророваться.|
|--threads THREADS|Количество потоков, которые будут выгружены на процессор. Если не указывать, будет выбрано автоматически, основываясь на количестве ядер ЦП.|
|--blasthreads [threads]|Количество слоёв, которые будут использоваться для BLAS. Если не указать, будет выбрано то же количество что и в --threads|
|--highpriority|Запускает кобольд в высоком приоритете, может повысить скорость генерации, но этот режим экспериментальный и может глючить.|
|  --contextsize 1024,2048,4096,...|Максимальный размер контекста, используйте только степени двойки. По дефолту 2048.|
|--blasbatchsize -1,32,64,...,2048|Размер пачки, которую единовременно будет обрабатывать BLAS. Значение -1 выключает BLAS вообще.|           
|--ropeconfig [rope-freq-scale] [[rope-freq-base] ...]|Скалирование ROPE и его база, например, --ropeconfig 0.25 10000. Если не указывать, автоматом выставит значения на основе твоего размера контекста. Для линейного масштабирования можно указать только первое значение.|
|--smartcontext|Зарезервирует порцию контекста чтобы пореже пересчитывать BLAS. Крайне хреново работает с memory, world info и author's note, потому что они все любят влезать посреди контекста, нарушая его целостность.|
|--noshift|Если указано, кобольд не будет пытаться отрезать или двигать контекст (это когда ты включаешь Allow Editing и удаляешь что-нибудь)|
|--bantokens [token_substrings] [[token_substrings] ...]|Банит указанные строки, сетка не будет их использовать.|
|--forceversion [version]|Если формат модели не определился сам собой, тут можно указать его, например, 401 для  GPTNeoX-Type2|
|--nommap|Не использовать mmap для загрузки новых моделей|
|--usemlock|Для эпплопидаров. Принудительно хранить модель в оперативке вместо того чтобы её двигать или сжимать.|
|--noavx2|Не использовать набор иструкций AVX2 (старые материнки и биосы не умеют в AVX2), не будет работать с --clblast|
|--debugmode [DEBUGMODE]|Режим отладки, выводит в консольку дополительную информацию.|
|--skiplauncher|Не открывает лаунчер кобольда, а сразу его запускает. Непонятно, нахуя это нужно, если при указании других параметров лаунчер всё равно не будет запускаться, а без них даже модельку не загрузить.|
|--hordeconfig [hordemodelname] [[hordegenlength] [hordemaxctx] [hordeapikey] [hordeworkername] ...]|Настройки для AI Horde. Первый параметр обязательный, остальные по желанию.|
|--noblas|Не использовать OpenBLAS для ускорения переваривания промпта.|
|--useclblast {0,1,..,8} {0,1,..,8}|Использовать CLBlast для ускорения при помощи видяхи. Нужно указать оба аргумента, первый это platform ID, второй device ID, например --useclblast 1 0).|
|--usecublas [[lowvram/normal] [main GPU ID] [mmq] [[lowvram/normal] [main GPU ID] [mmq] ...]]|Использовать CuBLAS для ускорения при помощи видяхи с CUDA. Выбирай lowvram чтобы не размечать VRAM под scratch buffer. Следующий параметр - ID видяхи, если не указать, будет использовать все видеокарты. Радеонщикам же нужно скачать форк кобольда под ROCm.|
|--gpulayers [GPU layers]|Количество слоёв, которые будут выгружены на видеокарту.|
|--tensor_split [Ratios] [[Ratios] ...]|Только для CUDA при выборе ALL GPU. Указывает пропорции для выгрузки тензоров по нескольким видяхам, например --tensor_split 7 3 выгрузит слои в соотношении семь к трём на первую и вторую видяху соответственно.|
|--onready [shell command]|После того как модель будет загружена, выполнит консольную команду, указанную в [shell command]|
|--multiuser [limit]|Запуск в режиме мультиюзера, который ставит в очередь приходящие запросы вместо того чтобы их игнорировать.|
|--remotetunnel|Запускает Cloudflare чтобы создать удалённый туннель, который позволит пользоваться кобольдом по сети, в том числе игнорируя фаерволлы. Можно врубить дома на пеке, пойти на пары и там крутить промпты с телефона.|
|--foreground|Будет вытаскивать окошко с консолькой на передний план после каждой генерации. Только для пользователей винды.|
|--preloadstory PRELOADSTORY|После запуска загрузит подготовленный в .json формате сценарий. Под сценариями тут подразумевается файлик, который ты можешь сохранить из веб-интерфейса кобольда.|
|--quiet|Тихий режим, скрывает вывод нагенерированного текста из консоли. Автоматически включается если указан --hordeconfig.|
|--ssl [cert_pem] [[key_pem] ...]|Позволяет выдавать весь производимый контент по SSL. Требуется указать действующий НЕЗАШИФРОВАННЫЙ SSL сертификат и ключ .pem|
         
</details>



#TODO:
Для удобства можно использовать интерфейс TavernAI или Text-Generation-WebUI

Описать режимы, Story / Adventure / Chat/ Instruction

Сделать небольшую сводку по семплерам и шизосемплингу

смартконтекст и контекст шифт
