# KoboldCpp

Гайд для ~~ретардов~~ быстрого вката в LLaMA без излишней ебли.

1. Качаешь [koboldcpp.exe последней версии](https://github.com/LostRuins/koboldcpp/releases/). 
2. Качаешь [одну из моделей](https://huggingface.co/TheBloke/Frostwind-10.7B-v1-GGUF/blob/main/frostwind-10.7b-v1.Q5_K_M.gguf)
3. Запускаем koboldcpp.exe и выбираем скачанную модель.
4. Заходим в браузере на <http://localhost:5001/>
Все, общаемся с ИИ, читаем охуительные истории или отправляемся в Adventure.

Да, просто запускаем, выбираем файл и открываем адрес в браузере, даже ваша бабка разберется!

# Какую модель качать?
Вывод кобольда (как и качество кумирования) целиком и полностью зависит от выбранной модели. Модель это плоть и кровь вашей нейроночки, это то, какими знаниями и навыками она обладает. Существуют как специализированные модели, так и общие, которые умеют всё понемногу. Есть даже модели, которые являются миксами двух и более существующих.

Модели делятся по **колличеству параметров** (в миллиардах, сокращённо "B") и по **степени квантования**.

**Количество параметров** приблизительно передаёт "вес" модели. Чем больше весит модель, тем она круче и умнее (но не всегда). Разница в качестве выдаваемого текста у 7B модели и 34B модели колоссальная, но вместе с обьёмом возрастает также и требования к памяти под модель, и к мощности пекарни, так что не удивляйтесь если на вашей 1050Ti моделька на 34B будет тратить на слово несколько секунд. На странице модели будет указано примерное потребление оперативки моделью. Эту память можно распределить между своей оперативкой и VRAM видеокарты. Говорят, если полностью загрузить модель в память видюхи, она будет работать побыстрее. Но даже если у вас памяти впритык, то всё равно можно попробовать, тогда модель будет использовать файл подкачки, из-за чего скорость упадёт в разы.

**Степень квантования** (чаще всего сокращают до Q_3 или Q5_K_M) это "сжатие модели", потипу lossy видеофайлов. Квантование может сильно уменьшить вес модели, при этом сохраняя её параметры, но модель от этого становится тупее. Чем меньше число после Q_, тем меньше квант, а значит, и меньше её качество. Иногда сильно квантованные модели просто неюзабельны из-за их тупости, как порнуха в 240p, однако на кванах 5 и больше потери качества не такие существенные. Говорят, на квантах Q_3 существует жизнь, но это не доказано.

Ну чтож, анон, давай выберем тебе модельку из самых популярных у анонов.  
- **Если всё супер плохо** и ты сидишь в деревне с бабушкиной пекарни для созвонов в скайпе, качай [OpenHermes-2.5-Mistral-7B](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/blob/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf).  
- **Если у тебя 8гб оперативы и видеокарта уровня 1050ti (или 16гб оперативы и нет видеокарты)**, качай [Frostwind-10.7B](https://huggingface.co/TheBloke/Frostwind-10.7B-v1-GGUF/blob/main/frostwind-10.7b-v1.Q5_K_M.gguf) или [OpenHermes-2.5-Mistral-7B](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/blob/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf).  
- **Если у тебя топовый егровой пека, 32гб оперативы и видеокарта уровня 3060TI/6600XT**, качай [Noromaid-20B](https://huggingface.co/TheBloke/Noromaid-20B-v0.1.1-GGUF/blob/main/noromaid-20b-v0.1.1.Q5_K_M.gguf) или [MLewd-ReMM-L2-Chat-20B](https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B-GGUF/blob/main/MLewd-ReMM-L2-Chat-20B.q5_K_M.gguf).  
- Для специализированных сборок с видюхами майнинг-уровня, вроде NVidia P40 24G можешь попробовать модельки на 70B. Они несколько круче 34B, но не сказать чтобы прям очень сильно, но зато тебе не придется ждать часами одного ответа.  
- Говорят, существует моделька на 180B, но, поверь, тебе это не нужно.

Если же ты решил сам выбрать себе модельку, ищи на huggingface файлы с расширением GGUF. Вот у этого челика есть [огромный список переконвертированных в GGUF моделей](https://huggingface.co/TheBloke?search_models=GGUF), пользуйся поиском.
А вот [страничка с рейтингом моделей для кумирования](http://ayumi.m8geil.de/ayumi_bench_v3_results.html), впрочем, основаны результаты не на отзывах юзеров, а _примерно почувствованы_, так что не панацея, спрашивай советов в треде.

И не забывай что иногда модель может показать себя очень круто, а иногда выдавать банальности и кринге, так что не дропай модели сразу после того как она обосрётся на поцелуе.

# Всё очень медленно работает!
При запуске KoboldCPP можно выбрать пресет для BLAS. BLAS — пакет библиотек, ответственный за "пережёвывание" твоего промпта (в том числе, разборка уже написаного текста на токены). Имеется несколько вариантов реализации BLAS:

По умолчанию выбран **openBLAS**. openBLAS это реализация BLAS для работы на CPU, и если у тебя нет видюхи (или по каким-то причинам ты не хочешь её использвать), выбирай его.

Если у тебя есть видеокарта, то для заметного ускорения работы, лучше использовать её. Ты можешь выгрузить на неё часть слоёв модели. Для этого в дропдауне окошка KoboldCPP, там, где написано openBLAS, нужно выбрать вариант под свою видюху.

Если у тебя NVidia, выберай **CuBLAS**. 
Если у тебя Radeon вместо видеокарты, есть смысл попробовать [СБОРКУ ПОД ROCm](https://github.com/YellowRoseCx/koboldcpp-rocm/releases), но ROCm под винду доступен только для 6800+ видеокарт. В списке появится **rocmBlas**, выбери его.

Если же у тебя радеон постарее, например 6600XT, выберай **clBlast**, он тоже будет использовать твою видеокарту, значительно ускорив работу нейронки.

Далее тебе нужно выбрать в каком слоте стоит видеокарта (в дропдауне, 1-4) и количество слоёв, которые туда нужно выгрузить. Выгруженные в видюху слои работают быстрее, а ещё они не жрут твою оперативу, позволяя тебе побыстрее генерить. Количество слоёв для выгрузки определяется индивидуально, экспериментами. В диспетчере задач погляди загруженность видюхи и выставляй больше или меньше по своему усмотрению. На нвидии, кстати, не стоит забивать память на фулл, говорят там будет всё глючить.

Если выбрать 0 слоёв для GPU, то текст писаться быстрее не будет, но на этапе precessing BLAT ускорение всё же будет.

# Сетка забыла что вообще происходит!
Так, смотри, память сетки ограничена размером контекста. Контекст это те слова (точнее, токены), которые ей скормлены. У большинства моделей для LLaMA2 максимальный размер контекста - 4096 токенов. Если ставить больше, сетка может ебануться и начать писать ахинею. Размер контекста ты самостоятельно выбираешь в лаунчере кобольда, чем он больше, тем больше он займёт оперативы, причем требования возрастают квадратично.

При повышении контекста, в зависимости от модели нужно будет изменять и один из параметров RoPE scale, или RoPE base. Некоторые файнтюны имеют кастомный RoPE base, который нужно выставлять таким, который указан в конфиге моделей, лежащем рядом с ней на обниморде. В ином же случае стоит придерживаться [значений из вики](https://github.com/LostRuins/koboldcpp/wiki#what-is-rope-config-what-is-ntk-aware-scaling--what-values-to-use-for-rope-config) для растягивания контекста, выше базовых 4к у ламы2.

Контекст это вообще всё что выгружено в память нейронки, на основе чего она генерирует слова. В него входит история сообщений в чате, **Memory**, **Author's Note** и **World Info**.

По мере чата/игры/написания истории сетка будет всё меньше и меньше обращать внимание на старые сообщения, а спустя некоторое время (после того как они окажутся дальше чем Context Size) они просто пропадут из её памяти.

Чтобы как-нибудь расширить память нейронки, существует вкладка **Memory**. В неё нужно коротко записывать основные моменты истории, чтобы не лишать аниме-тянок девственности по два раза за игру. Мемори запихивается в самое начало контекста и имеет довольно мало влияния на генерацию.

Также чтобы сетка запомнила, например, о том как выглядит и какой характер у той или иной тянки, или чтобы доучить её какому-нибудь понятию о мире, которого в ней изначально не было, нужно использовать **World Info**. На этой вкладке ты указываешь слова и их ассоциации для нейронки. Например, ты можешь создать ключ "Melissa" и описать его как "Melissa is a petite 19-year old girl wearing glasses with irritated behavior. She often gets angry at everybody but can me easily calmed. Melissa and Kat are best friends.". Теперь, после упоминания Мелиссы сетка получит описанные токены и будет генерировать текст на основе них. В описание ключа также нужно вставить само имя ключа, то есть не "A girl..." а "Melissa is a girl...", так как сеть не включает само название ключа в контекст.

Ну и самое важное, **Author's Note** это такое поле, которое будет кормиться сетке перед каждым запросом. То есть, буквально, напоминать ей что именно от неё ждут, в целом. Тут лучше всего написать какой должен быть стиль повествования, краткие сведения о мире и прям совсем общие положения, например, "The horror story where the young campers are being hunted and killed in the woods by an unknown maniac". Также, у Author's note можно указать дальность от текста, которая работает как "Важность" указанной информации.

[>> Подробнее про долговременную память](https://github.com/KoboldAI/KoboldAI-Client/wiki/Memory,-Author's-Note-and-World-Info)


#TODO:
Для удобства можно использовать интерфейс TavernAI или Text-Generation-WebUI

Описать режимы, Story / Adventure / Chat/ Instruction

Сделать небольшую сводку по семплерам и шизосемплингу

Консольные команды для быстрого запуска

смартконтекст и контекст шифт
