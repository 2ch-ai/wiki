# KoboldCpp

Гайд для ~~ретардов~~ быстрого вката в LLaMA без излишней ебли.

1. Качаешь [koboldcpp.exe последней версии](https://github.com/LostRuins/koboldcpp/releases/). 
2. Качаешь [одну из моделей](https://huggingface.co/TheBloke/Frostwind-10.7B-v1-GGUF/blob/main/frostwind-10.7b-v1.Q5_K_M.gguf)
3. Запускаем koboldcpp.exe и выбираем скачанную модель.
4. Заходим в браузере на <http://localhost:5001/>
Все, общаемся с ИИ, читаем охуительные истории или отправляемся в Adventure.

Да, просто запускаем, выбираем файл и открываем адрес в браузере, даже ваша бабка разберется!

# Какую модель качать?
Вывод кобольда (как и качество кумирования) целиком и полностью зависит от выбранной модели. Модель это плоть и кровь вашей нейроночки, это то, какими знаниями и навыками она обладает. Существуют как специализированные модели, так и общие, которые умеют всё понемногу. Есть даже модели, которые являются миксами двух и более существующих.

Модели делятся по **колличеству параметров** (в миллиардах, сокращённо "B") и по **степени квантования**.

**Количество параметров** приблизительно передаёт "вес" модели. Чем больше весит модель, тем она круче и умнее (но не всегда). Разница в качестве выдаваемого текста у 7B модели и 34B модели колоссальная, но вместе с обьёмом возрастает также и требования к памяти под модель, и к мощности пекарни, так что не удивляйтесь если на вашей 1050Ti моделька на 34B будет тратить на слово несколько секунд. На странице модели будет указано примерное потребление оперативки моделью. Эту память можно распределить между своей оперативкой и VRAM видеокарты. Говорят, если полностью загрузить модель в память видюхи, она будет работать побыстрее. Но даже если у вас памяти впритык, то всё равно можно попробовать, тогда модель будет использовать файл подкачки, из-за чего скорость упадёт в разы.

**Степень квантования** (чаще всего сокращают до Q_3 или Q5_K_M) это "сжатие модели", потипу lossy видеофайлов. Квантование может сильно уменьшить вес модели, при этом сохраняя её параметры, но модель от этого становится тупее. Чем меньше число после Q_, тем меньше квант, а значит, и меньше её качество. Иногда сильно квантованные модели просто неюзабельны из-за их тупости, как порнуха в 240p, однако на кванах 5 и больше потери качества не такие существенные. Говорят, на квантах Q_3 существует жизнь, но это не доказано.

Ну чтож, анон, давай выберем тебе модельку из самых популярных у анонов.  
- **Если всё супер плохо** и ты сидишь в деревне с бабушкиной пекарни для созвонов в скайпе, качай [OpenHermes-2.5-Mistral-7B](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/blob/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf).  
- **Если у тебя 8гб оперативы и видеокарта уровня 1050ti (или 16гб оперативы и нет видеокарты)**, качай [Frostwind-10.7B](https://huggingface.co/TheBloke/Frostwind-10.7B-v1-GGUF/blob/main/frostwind-10.7b-v1.Q5_K_M.gguf) или [OpenHermes-2.5-Mistral-7B](https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/blob/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf).  
- **Если у тебя топовый егровой пека, 32гб оперативы и видеокарта уровня 3060TI/6600XT**, качай [Noromaid-20B](https://huggingface.co/TheBloke/Noromaid-20B-v0.1.1-GGUF/blob/main/noromaid-20b-v0.1.1.Q5_K_M.gguf) или [MLewd-ReMM-L2-Chat-20B](https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B-GGUF/blob/main/MLewd-ReMM-L2-Chat-20B.q5_K_M.gguf).  
- Для специализированных сборок с видюхами майнинг-уровня, вроде NVidia P40 24G можешь попробовать модельки на 70B. Они несколько круче 34B, но не сказать чтобы прям очень сильно, но зато тебе не придется ждать часами одного ответа.  
- Говорят, существует моделька на 180B, но, поверь, тебе это не нужно.

Если же ты решил сам выбрать себе модельку, ищи на huggingface файлы с расширением GGUF. Вот у этого челика есть [огромный список переконвертированных в GGUF моделей](https://huggingface.co/TheBloke?search_models=GGUF), пользуйся поиском.
А вот [страничка с рейтингом моделей для кумирования](http://ayumi.m8geil.de/ayumi_bench_v3_results.html), впрочем, основаны результаты не на отзывах юзеров, а _примерно почувствованы_, так что не панацея, спрашивай советов в треде.

И не забывай что иногда модель может показать себя очень круто, а иногда выдавать банальности и кринге, так что не дропай модели сразу после того как она обосрётся на поцелуе.

# Всё очень медленно работает!
При запуске KoboldCPP можно выбрать пресет для BLAS. BLAS — пакет библиотек, ответственный за "пережёвывание" твоего промпта (в том числе, разборка уже написаного текста на токены). Имеется несколько вариантов реализации BLAS:

По умолчанию выбран **openBLAS**. openBLAS это реализация BLAS для работы на CPU, и если у тебя нет видюхи (или по каким-то причинам ты не хочешь её использвать), выбирай его.

Если у тебя есть видеокарта, то для заметного ускорения работы, лучше использовать её. Ты можешь выгрузить на неё часть слоёв модели. Для этого в дропдауне окошка KoboldCPP, там, где написано openBLAS, нужно выбрать вариант под свою видюху.

Если у тебя NVidia, выберай **CuBLAS**. 
Если у тебя Radeon вместо видеокарты, есть смысл попробовать [СБОРКУ ПОД ROCm](https://github.com/YellowRoseCx/koboldcpp-rocm/releases), но ROCm под винду доступен только для 6800+ видеокарт. В списке появится **rocmBlas**, выбери его.

Если же у тебя радеон постарее, например 6600XT, выберай **clBlast**, он тоже будет использовать твою видеокарту, значительно ускорив работу нейронки.

Далее тебе нужно выбрать в каком слоте стоит видеокарта (в дропдауне, 1-4) и количество слоёв, которые туда нужно выгрузить. Выгруженные в видюху слои работают быстрее, а ещё они не жрут твою оперативу, позволяя тебе побыстрее генерить. Количество слоёв для выгрузки определяется индивидуально, экспериментами. В диспетчере задач погляди загруженность видюхи и выставляй больше или меньше по своему усмотрению. На нвидии, кстати, не стоит забивать память на фулл, говорят там будет всё глючить.

Если выбрать 0 слоёв для GPU, то текст писаться быстрее не будет, но на этапе precessing BLAT ускорение всё же будет.

# Сетка забыла что вообще происходит!
Так, смотри, память сетки ограничена размером контекста. Контекст это те слова (точнее, токены), которые ей скормлены. У большинства моделей для LLaMA2 максимальный размер контекста - 4096 токенов. Если ставить больше, сетка может ебануться и начать писать ахинею. Размер контекста ты самостоятельно выбираешь в лаунчере кобольда, чем он больше, тем больше он займёт оперативы, причем требования возрастают квадратично.

При повышении контекста, в зависимости от модели нужно будет изменять и один из параметров RoPE scale, или RoPE base. Некоторые файнтюны имеют кастомный RoPE base, который нужно выставлять таким, который указан в конфиге моделей, лежащем рядом с ней на обниморде. В ином же случае стоит придерживаться [значений из вики](https://github.com/LostRuins/koboldcpp/wiki#what-is-rope-config-what-is-ntk-aware-scaling--what-values-to-use-for-rope-config) для растягивания контекста, выше базовых 4к у ламы2.

Контекст это вообще всё что выгружено в память нейронки, на основе чего она генерирует слова. В него входит история сообщений в чате, **Memory**, **Author's Note** и **World Info**.

По мере чата/игры/написания истории сетка будет всё меньше и меньше обращать внимание на старые сообщения, а спустя некоторое время (после того как они окажутся дальше чем Context Size) они просто пропадут из её памяти.

Чтобы как-нибудь расширить память нейронки, существует вкладка **Memory**. В неё нужно коротко записывать основные моменты истории, чтобы не лишать аниме-тянок девственности по два раза за игру. Мемори запихивается в самое начало контекста и имеет довольно мало влияния на генерацию.

Также чтобы сетка запомнила, например, о том как выглядит и какой характер у той или иной тянки, или чтобы доучить её какому-нибудь понятию о мире, которого в ней изначально не было, нужно использовать **World Info**. На этой вкладке ты указываешь слова и их ассоциации для нейронки. Например, ты можешь создать ключ "Melissa" и описать его как "Melissa is a petite 19-year old girl wearing glasses with irritated behavior. She often gets angry at everybody but can me easily calmed. Melissa and Kat are best friends.". Теперь, после упоминания Мелиссы сетка получит описанные токены и будет генерировать текст на основе них. В описание ключа также нужно вставить само имя ключа, то есть не "A girl..." а "Melissa is a girl...", так как сеть не включает само название ключа в контекст.

Ну и самое важное, **Author's Note** это такое поле, которое будет кормиться сетке перед каждым запросом. То есть, буквально, напоминать ей что именно от неё ждут, в целом. Тут лучше всего написать какой должен быть стиль повествования, краткие сведения о мире и прям совсем общие положения, например, "The horror story where the young campers are being hunted and killed in the woods by an unknown maniac". Также, у Author's note можно указать дальность от текста, которая работает как "Важность" указанной информации.

[>> Подробнее про долговременную память](https://github.com/KoboldAI/KoboldAI-Client/wiki/Memory,-Author's-Note-and-World-Info)

#Мне лень каждый раз выбирать нужное в лаунчере

Итак, ты уже разобрался с лаунчером, знаешь где что тыкать, какую модельку загружать, сколько слоёв куда ставить. Давай ускорим запуск кобольда с помощью командной строки.

Значит так, жмёшь пкм на ярлыке кобольда и дописываешь в поле "Обьект" что-то типа следующего:`C:\NeuralNet\LLaMA\koboldcpp.exe --model frostwind-10.7b-v1.Q5_K_M.gguf --port 5001`, это запустит консольку кобольда сразу же, без запуска UI, загрузит модель в неё и начнет работу. Все параметры, которые ты вбиваешь в UI, можно указать и в консоли. Например, запуск кобольда с параметрами `C:\NeuralNet\LLaMA\koboldcpp.exe --model frostwind-10.7b-v1.Q5_K_M.gguf --port 5001 --contextsize 4096  --useclblast 0 1  --gpulayers 28` загрузит модель Frostwind, откроет под себя порт localhost:5001, задаст размер контекста в 4096, будет использовать clBlast (тут методом тыка нужно выяснить где где стоит видюха, можешь попробовать 0 0 или 1 0, пробуй пока не она не определится), и выгрузит на неё 28 слоёв. Удобно же.

Список команд можно подсмотреть запустив кобольд из консоли с командой --help. Вот он: [Полный список консольных команд для KoboldCPP](https://2ch-ai.github.io/wiki/llama/guides/kobold-cpp/kobold-cpp-consolecommands/)

##А может робот написать мне историю, создать шедевр?
Может! И даже больше — он может поиграть с тобой в текстовую РПГ.

У интерфейса KoboldCPP есть 4 режима, их можно выбрать в в настройках, переключаясь между Story/Adventure/Chat/Instruction или нажать кнопку "Scenarios" вверху страницы чтобы выбрать один из заготовленных шаблонов сценария (или даже загрузить его с внешнего сайта).

###Story Mode
Режим истории будет создавать тебе историю, основанную на твоем первом промпте. Например, вот таком: `Write a romantic story about a japanese schoolboy kissed a girl for the first time in his life. The story starts in the morning in school classroom.`, и ИИ начнет писать тебе историю, кусками по 120 токенов (или сколько ты задашь в настройках). Не забывай про Author's note, это очень важный и мощный инструмент, который втыкает часть инфы посреди контекста, задавая тон повествования или важные ньюансы, которые ты бы хотел видеть в своей истории.

###Adventure Mode
Режим приключения это, по сути, текстовая игра, в которой ты, пользователь, сам можешь выбирать что и когда сделать или сказать. Сетка будет выдавать тебе абзацы текста на основе твоих ответов, описывая происходящие события или последствия твоих ответов. Например, можешь исекайнуться в фентези манямирок или побывать в шкуре обычного японского школьника, подкатывающего к одноклассницам. Возможности околобезграничны при использовании правильного промпта и Author's note. Например, `Write a thrilling story about a group of university students are lost in the woods on a hiking trip. Player is one of the male students. Students may become badly wounded or even die from natural dangers of the woods. The story starts when all of the students are gathered around a campfire.`.
Используй ответы вроде `> describe scenario in details` чтобы сетка описала сценарий с самого начала, ввела тебя в курс дела. `> describe Melissa's look and personality` опишет встреченного человечка, а `> try to defend your friends with a flaming stick` поможет тебе одолеть волков/гоблинов/маньяков. Чтобы разговаривать с персонажами, просто пиши ответ в кавычках, вроде `> "Oh, come on, Melissa, stop being such a nerd! Here, have a drink."`.

###Chat Mode
Ну, это классический чат с лламой. Опиши персонажа в Memory или в Author's note в третьем лице, например `[Character: Kat; age: 20; gender: female; Kat is a rebellius university student who is cheerful and love to play guitar and drinking beer.][The following is a chat message log between Kat and you.]`. Всё, можешь начинать болтать. Функционал для чатинга в кобольде ограничен, например тут нет возможности описать своего аватара, и нет примеров сообщений, так что для комфортного чятинга советую скачать SillyTavern и поставть её.

###Instruction Mode
Режим инструкций нужен для того чтобы просить ИИ сделать что-то нужное тебе. Например, оформить разметку JSON или написать стишок. У режима инструкций нет конкретной задачи, тебе нужно формулировать её самостоятельно. Просто введи `{{Input}}` и укажи свой запрос, закончив его `{{Output}}`, и сетка выдаст тебе ответ. У разных моделей разные форматы Input и Output, подсмотреть можно у Bloke или на странице модели на huggingface. В настройках нужно указать вид форматов инструкций на котором работает модель, у большинства моделей сейчас формат Alpaca.

#TODO:

Для удобства можно использовать интерфейс TavernAI или Text-Generation-WebUI

Сделать небольшую сводку по семплерам и шизосемплингу

смартконтекст и контекст шифт
