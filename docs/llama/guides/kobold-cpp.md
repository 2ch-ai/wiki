# KoboldCpp

Гайд для ~~ретардов~~ быстрого вката в LLaMA без излишней ебли.
Грузит всё в процессор, поэтому ёба карта не нужна, запаситесь оперативкой и подкачкой.

1. Скачиваем [koboldcpp.exe последней версии](https://github.com/LostRuins/koboldcpp/releases/). 
2. Скачиваем модель в gguf формате. Например вот эту:
<https://huggingface.co/Undi95/MLewd-ReMM-L2-Chat-20B-GGUF/blob/main/MLewd-ReMM-L2-Chat-20B.q5_K_M.gguf>
Если совсем бомж и капчуешь с микроволновки, то можно взять
<https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF/blob/main/openhermes-2.5-mistral-7b.Q5_K_M.gguf>
Можно просто вбить в huggingace в поиске "gguf" и скачать любую, охуеть, да? Главное, скачай файл с расширением .gguf, а не какой-нибудь .pt
3. Запускаем koboldcpp.exe и выбираем скачанную модель.
4. Заходим в браузере на <http://localhost:5001/>
5. Все, общаемся с ИИ, читаем охуительные истории или отправляемся в Adventure.

Да, просто запускаем, выбираем файл и открываем адрес в браузере, даже ваша бабка разберется!

Для удобства можно использовать интерфейс TavernAI

# Всё очень медленно работает!
Если у тебя есть видеокарта, можешь выгрузить на неё часть слоёв модели. Для этого в дропдауне окошка KoboldCPP, там, где написано openBLAS, нужно выбрать вариант под свою видюху.

Если у тебя NVidia, выберай **CuBLAS**. 
Если у тебя Radeon вместо видеокарты, есть смысл попробовать [СБОРКУ ПОД ROCm](https://github.com/YellowRoseCx/koboldcpp-rocm/releases), но ROCm под винду доступен только для 6800+ видеокарт. В списке появится **rocmBlas**, выбери его.

Если же у тебя радеон постарее, например 6600XT, выберай **clBlast**, он тоже будет использовать твою видеокарту, значительно ускорив работу нейронки.

Далее тебе нужно выбрать в каком слоте стоит видеокарта (в дропдауне, 1-4) и количество слоёв, которые туда нужно выгрузить. Выгруженные в видюху слои работают быстрее, а ещё они не жрут твою оперативу, позволяя тебе побыстрее генерить. Количество слоёв для выгрузки определяется индивидуально, экспериментами. В диспетчере задач погляди загруженность видюхи и выставляй больше или меньше по своему усмотрению.

# Сетка забыла что вообще происходит!
Так, смотри, память сетки ограничена размером контекста. Контекст это те слова (точнее, токены), которые ей скормлены. У большинства моделей для LLaMA2 максимальный размер контекста - 4096 токенов. Если ставить больше, сетка может ебануться и начать писать ахинею. Размер контекста ты самостоятельно выбираешь в лаунчере кобольда, чем он больше, тем больше он займёт оперативы, причем требования возрастают квадратично.

При повышении контекста, в зависимости от модели нужно будет изменять и один из параметров RoPE scale, или RoPE base. Некоторые файнтюны имеют кастомный RoPE base, который нужно выставлять таким, который указан в конфиге моделей, лежащем рядом с ней на обниморде. В ином же случае стоит придерживаться [значений из вики](https://github.com/LostRuins/koboldcpp/wiki#what-is-rope-config-what-is-ntk-aware-scaling--what-values-to-use-for-rope-config) для растягивания контекста, выше базовых 4к у ламы2.

Контекст это вообще всё что выгружено в память нейронки, на основе чего она генерирует слова. В него входит история сообщений в чате, **Memory**, **Author's Note** и **World Info**.

По мере чата/игры/написания истории сетка будет всё меньше и меньше обращать внимание на старые сообщения, а спустя некоторое время (после того как они окажутся дальше чем Context Size) они просто пропадут из её памяти.

Чтобы как-нибудь расширить память нейронки, существует вкладка **Memory**. В неё нужно коротко записывать основные моменты истории, чтобы не лишать аниме-тянок девственности по два раза за игру. Мемори запихивается в самое начало контекста и имеет довольно мало влияния на генерацию.

Также чтобы сетка запомнила, например, о том как выглядит и какой характер у той или иной тянки, или чтобы доучить её какому-нибудь понятию о мире, которого в ней изначально не было, нужно использовать **World Info**. На этой вкладке ты указываешь слова и их ассоциации для нейронки. Например, ты можешь создать ключ "Melissa" и описать его как "Melissa is a petite 19-year old girl wearing glasses with irritated behavior. She often gets angry at everybody but can me easily calmed. Melissa and Kat are best friends.". Теперь, после упоминания Мелиссы сетка получит описанные токены и будет генерировать текст на основе них. В описание ключа также нужно вставить само имя ключа, то есть не "A girl..." а "Melissa is a girl...", так как сеть не включает само название ключа в контекст.

Ну и самое важное, **Author's Note** это такое поле, которое будет кормиться сетке перед каждым запросом. То есть, буквально, напоминать ей что именно от неё ждут, в целом. Тут лучше всего написать какой должен быть стиль повествования, краткие сведения о мире и прям совсем общие положения, например, "The horror story where the young campers are being hunted and killed in the woods by an unknown maniac". Также, у Author's note можно указать дальность от текста, которая работает как "Важность" указанной информации.

[>> Подробнее про долговременную память](https://github.com/KoboldAI/KoboldAI-Client/wiki/Memory,-Author's-Note-and-World-Info)
